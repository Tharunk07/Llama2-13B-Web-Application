<h1>Llama-2-13B Model</h1>

The Llama-2-13B model is a machine learning model deployed on the Google Cloud Platform (GCP) cloud platform. It is accessed locally through a user-friendly interface created with Streamlit, allowing users to send requests and receive responses from the model.

<h2>Technologies Used</h2>

**Python** : The primary programming language used for developing the model and the interface.  
**google-auth** : This library is utilized for authenticating with Google Cloud services.  
**Google Cloud Service** : The Google Cloud Platform is used for hosting and deploying the model.  
**Streamlit** : Streamlit is employed for creating the interactive web interface for accessing the model.  

<h2>Getting Started</h2>
To get started with using the Llama-2-13B model, follow these steps:

**Installation**: Ensure you have Python installed on your local system. Use pip to install the necessary dependencies:
```bash
pip install -r requirements.txt
```
**Deployment Information**

The Llama model has been privately deployed, and access is available upon request.
To request access to the deployed Llama model and its private keys, please contact:

- **Email:** [tharunkkumarasamy@gmail.com]
Please provide details about your use case and any specific requirements when contacting for access.

Once access is granted, you will proceed with further steps.

**Run the Streamlit App**: Execute the Streamlit app to access the model interface locally:
```bash
streamlit run app.py
```
**Access the Model**: Once the Streamlit app is running, open a web browser and navigate to the provided URL to access the interface. From there, you can send requests to the model and view the responses.
